{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7_1_InstanceSegmentation_Detectron2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMcjXYxuXBAY84IWaRFJ5Uj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<div style=\"width: 100%; clear: both;\">\n","<div style=\"float: left; width: 50%;\">\n","<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n","</div>\n","<div style=\"float: right; width: 50%;\">\n","<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M0.532 Â· Pattern Recognition</p>\n","<p style=\"margin: 0; text-align:right;\">Computational Engineering and Mathematics Master</p>\n","<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Computers, Multimedia and Telecommunications Department</p>\n","</div>\n","</div>\n","<div style=\"width:100%;\">&nbsp;</div>"],"metadata":{"id":"HenFpZYvn-xS"}},{"cell_type":"markdown","metadata":{"id":"0AbgKG7XhG9F"},"source":["In this notebook we will see how to use the Mask R-CNN model for inference and training on the [Detectron2 framework](https://github.com/facebookresearch/detectron2). This notebook is the same as the one we already saw for object detection since it deals with both object detection and instance segmentation tasks.\n","\n","\n","\n"]},{"cell_type":"markdown","source":["Let's start by installing the Detectron2 framework."],"metadata":{"id":"CD9qXi-py8o3"}},{"cell_type":"code","metadata":{"id":"uVc2iO69hDdy"},"source":["!pip install pyyaml==5.1\n","\n","import torch\n","TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n","CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n","print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n","# Install detectron2 that matches the above pytorch version\n","# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n","# If there is not yet a detectron2 release that matches the given torch + CUDA version, you need to install a different pytorch.\n","\n","# exit(0)  # After installation, you may need to \"restart runtime\" in Colab. This line can also restart runtime"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We also need to import some common libraries and detectron2 utilities."],"metadata":{"id":"isqx2fL9zCfK"}},{"cell_type":"code","source":["# Some basic setup:\n","# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# import some common libraries\n","import numpy as np\n","import os, json, cv2, random\n","from google.colab.patches import cv2_imshow\n","\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog"],"metadata":{"id":"T2YksgI04Xk5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Inference with a pre-trained model"],"metadata":{"id":"8HWFIAwD4mi4"}},{"cell_type":"markdown","source":["On this part of the notebook, we will see how to use a pretrained model for inference. Let's start by downloading and showing a sample image that will be used for object detection and segmentation."],"metadata":{"id":"qWXDvCgUzNmD"}},{"cell_type":"code","source":["!wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg\n","im = cv2.imread(\"./input.jpg\")\n","cv2_imshow(im)"],"metadata":{"id":"mjc3KDJS4b8V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We need to set different configuration parameters. Here we will use the Mask R-CNN model with a ResNet-50 backbone and a Feature Pyramid Network (FPN) from the model zoo. We also define a threshold to remove all those predictions with a low confidence score. We use the DefaultPredictor and we pass the input image through it to get the output predictions."],"metadata":{"id":"IkC4baeNzROR"}},{"cell_type":"code","source":["cfg = get_cfg()\n","# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n","# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n","predictor = DefaultPredictor(cfg)\n","outputs = predictor(im)"],"metadata":{"id":"ebqE_Oz-4kwi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can take a look at the outputs by printing the predicted classes and the predicted bounding boxes."],"metadata":{"id":"5vIryTmwzVpo"}},{"cell_type":"code","source":["# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n","print(outputs[\"instances\"].pred_classes)\n","print(outputs[\"instances\"].pred_boxes)"],"metadata":{"id":"5RIeT__y4w4g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also use the Visualizer to show the predictions on top of the image."],"metadata":{"id":"oYXiBVtZzZaj"}},{"cell_type":"code","source":["# We can use `Visualizer` to draw the predictions on the image.\n","v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n","out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","cv2_imshow(out.get_image()[:, :, ::-1])"],"metadata":{"id":"vj2qZaLw400l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training on a custom dataset"],"metadata":{"id":"L4zAdLJg44dY"}},{"cell_type":"markdown","source":["Now, let's see how we train the same model but on a custom dataset. We will use a dataset of images with balloons, a category which is not among the COCO dataset categories."],"metadata":{"id":"ts8DYQ1GzfMk"}},{"cell_type":"code","source":["# download, decompress the data\n","!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\n","!unzip balloon_dataset.zip > /dev/null"],"metadata":{"id":"SvVYrpqH468H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As commented in the first lines of the code, if the dataset is in COCO format, the use of a custom dataset is much easier and only three lines of code are enough to use it in the Detectron2 framework (the ones commented at the beginning). If it is not the case, as the balloon dataset that we are using in this notebook, then we need to transform the annotations to COCO format with the following implementation.\n","\n","Notice that although we are considering the problem of object detection, we are dealing here a problem that handles both object detection and instance segmentation."],"metadata":{"id":"tdwlljk6ziSl"}},{"cell_type":"code","source":["# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n","# from detectron2.data.datasets import register_coco_instances\n","# register_coco_instances(\"my_dataset_train\", {}, \"json_annotation_train.json\", \"path/to/image/dir\")\n","# register_coco_instances(\"my_dataset_val\", {}, \"json_annotation_val.json\", \"path/to/image/dir\")\n","\n","from detectron2.structures import BoxMode\n","\n","def get_balloon_dicts(img_dir):\n","    json_file = os.path.join(img_dir, \"via_region_data.json\")\n","    with open(json_file) as f:\n","        imgs_anns = json.load(f)\n","\n","    dataset_dicts = []\n","    for idx, v in enumerate(imgs_anns.values()):\n","        record = {}\n","        \n","        filename = os.path.join(img_dir, v[\"filename\"])\n","        height, width = cv2.imread(filename).shape[:2]\n","        \n","        record[\"file_name\"] = filename\n","        record[\"image_id\"] = idx\n","        record[\"height\"] = height\n","        record[\"width\"] = width\n","      \n","        annos = v[\"regions\"]\n","        objs = []\n","        for _, anno in annos.items():\n","            assert not anno[\"region_attributes\"]\n","            anno = anno[\"shape_attributes\"]\n","            px = anno[\"all_points_x\"]\n","            py = anno[\"all_points_y\"]\n","            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n","            poly = [p for x in poly for p in x]\n","\n","            obj = {\n","                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n","                \"bbox_mode\": BoxMode.XYXY_ABS,\n","                \"segmentation\": [poly],\n","                \"category_id\": 0,\n","            }\n","            objs.append(obj)\n","        record[\"annotations\"] = objs\n","        dataset_dicts.append(record)\n","    return dataset_dicts\n","\n","for d in [\"train\", \"val\"]:\n","    DatasetCatalog.register(\"balloon_\" + d, lambda d=d: get_balloon_dicts(\"balloon/\" + d))\n","    MetadataCatalog.get(\"balloon_\" + d).set(thing_classes=[\"balloon\"])\n","balloon_metadata = MetadataCatalog.get(\"balloon_train\")"],"metadata":{"id":"ulyrURu34-L7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's visualize three random samples from the training set with the corresponding ground truth categories, bounding boxes and segmentation masks."],"metadata":{"id":"fNWaQSn-zm2P"}},{"cell_type":"code","source":["dataset_dicts = get_balloon_dicts(\"balloon/train\")\n","for d in random.sample(dataset_dicts, 3):\n","    img = cv2.imread(d[\"file_name\"])\n","    visualizer = Visualizer(img[:, :, ::-1], metadata=balloon_metadata, scale=0.5)\n","    out = visualizer.draw_dataset_dict(d)\n","    cv2_imshow(out.get_image()[:, :, ::-1])"],"metadata":{"id":"wX9VddMq5DKl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once we have the dataset loader ready for our custom dataset, we set all the configuration parameters for training. We specify the model used for training (Mask R-CNN with ResNet50 backbone and Feature Pyramid Network) and the number of classes for our classification problem (1, we only want to detect and segment balloons) among other typical training parameters (learning rate, batch size, etc.). We define an instance trainer of the DefaultTrainer class and we call the method train() to perform the training."],"metadata":{"id":"ZZhlXw-7zsU1"}},{"cell_type":"code","source":["from detectron2.engine import DefaultTrainer\n","\n","cfg = get_cfg()\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n","cfg.DATASETS.TRAIN = (\"balloon_train\",)\n","cfg.DATASETS.TEST = ()\n","cfg.DATALOADER.NUM_WORKERS = 2\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n","cfg.SOLVER.IMS_PER_BATCH = 2\n","cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n","cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n","cfg.SOLVER.STEPS = []        # do not decay learning rate\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n","# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n","\n","os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","trainer = DefaultTrainer(cfg) \n","trainer.resume_or_load(resume=False)\n","trainer.train()"],"metadata":{"id":"iQcv-cz65G9h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can use the tensorboard extension to visualize the training curves."],"metadata":{"id":"GzOikA23zw1Z"}},{"cell_type":"code","source":["# Look at training curves in tensorboard:\n","%load_ext tensorboard\n","%tensorboard --logdir output"],"metadata":{"id":"o3g25meD5LDU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once the model has been trained, we can use it for inference by defining an instance predictor of the DefaultPredictor class."],"metadata":{"id":"n2fcmRi4z1XP"}},{"cell_type":"code","source":["# Inference should use the config with parameters that are used in training\n","# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n","cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n","predictor = DefaultPredictor(cfg)"],"metadata":{"id":"7VsyGRtt5VFz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We use the predictor to get the predictions from some random images of the validation subset. We visualize these predictions on top of the images with the corresponding predicted categories, bounding boxes and segmentation masks."],"metadata":{"id":"IdJtnN9jz54c"}},{"cell_type":"code","source":["from detectron2.utils.visualizer import ColorMode\n","dataset_dicts = get_balloon_dicts(\"balloon/val\")\n","for d in random.sample(dataset_dicts, 3):    \n","    im = cv2.imread(d[\"file_name\"])\n","    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n","    v = Visualizer(im[:, :, ::-1],\n","                   metadata=balloon_metadata, \n","                   scale=0.5, \n","                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n","    )\n","    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","    cv2_imshow(out.get_image()[:, :, ::-1])"],"metadata":{"id":"_hspwC3c5crD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we can also proceed to evaluate the model trained over the whole validation subset by using the COCOEvaluator class to obtain some quantitative results."],"metadata":{"id":"2uocu8Viz_rV"}},{"cell_type":"code","source":["from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","from detectron2.data import build_detection_test_loader\n","evaluator = COCOEvaluator(\"balloon_val\", output_dir=\"./output\")\n","val_loader = build_detection_test_loader(cfg, \"balloon_val\")\n","print(inference_on_dataset(predictor.model, val_loader, evaluator))\n","# another equivalent way to evaluate the model is to use `trainer.test`"],"metadata":{"id":"oM8FJuzw5kBU"},"execution_count":null,"outputs":[]}]}