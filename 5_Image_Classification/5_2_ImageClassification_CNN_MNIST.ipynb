{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_2_ImageClassification_CNN_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"width: 100%; clear: both;\">\n",
        "<div style=\"float: left; width: 50%;\">\n",
        "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
        "</div>\n",
        "<div style=\"float: right; width: 50%;\">\n",
        "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M0.532 Â· Pattern Recognition</p>\n",
        "<p style=\"margin: 0; text-align:right;\">Computational Engineering and Mathematics Master</p>\n",
        "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Computers, Multimedia and Telecommunications Department</p>\n",
        "</div>\n",
        "</div>\n",
        "<div style=\"width:100%;\">&nbsp;</div>"
      ],
      "metadata": {
        "id": "4ZE9xgaZnGq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Classification with Convolutional Neural Networks (CNNs): Training from scratch\n",
        "\n",
        "In this notebook we will define a CNN architecture which will be trained from scratch on a dataset of handwritten digits. "
      ],
      "metadata": {
        "id": "xbNieYVYaoJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first import several modules from torch library which will be used for defining our CNN architecture."
      ],
      "metadata": {
        "id": "hUqJIWASbMiU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAK2qgX-4J-U"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define the batch size for training and testing. We also define the transformations that will be applied to the images (ToTensor and Normalize). Finally, we prepare the training and test dataset loaders from the MNIST dataset."
      ],
      "metadata": {
        "id": "qshZoGXibZhI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjJdUS659R-f"
      },
      "source": [
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "\n",
        "\n",
        "data_transform = torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST('/files/', train=True, download=True, transform=data_transform), \n",
        "    batch_size=batch_size_train, \n",
        "    shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=False, download=True, transform=data_transform), \n",
        "    batch_size=batch_size_test, \n",
        "    shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how it is a test batch. We can see how each batch is a Tensor with 4 dimensions, where the first dimension is the batch size (1000), the second dimension is the number of channels of the image (1, i.e. gray-scale image), and the third and fourth dimensions correspond to the image size (28x28 pixels). We also check the example_targets variable, which corresponds to the labels, with a size equals to the batch size (1000)."
      ],
      "metadata": {
        "id": "EegEMf0McSdn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k0HkC6M_Y9T"
      },
      "source": [
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "\n",
        "print('batch size {}, canals {}, ample {}, alt {}'.format(example_data.shape[0], example_data.shape[1], example_data.shape[2], example_data.shape[3]))\n",
        "print('Labels {}'.format(example_targets.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We show next some image samples from MNIST dataset with their corresponding ground truth labels."
      ],
      "metadata": {
        "id": "gf72mHX3d98U"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1iMJo1J_gx8"
      },
      "source": [
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once explored the MNIST dataset and defined the dataset loaders, we also need to define the architecture that will be used to train the model. Here, we define an architecture which consists of two convolutional layers with 5x5 kernels, followed by one Dropout layer, and finally two fully connected layers. The last fully connected layer (fc2) has as many output units and the number of categories for the image classification problem (10). Whereas in the $__init__$ function we define the different layers from our neural network architecture, in the forward function, we specify how the data will pass through our model."
      ],
      "metadata": {
        "id": "-FOM0hQGeLpV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3uxuqSv_qIo"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        num_classes = 10\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, num_classes)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the values for some training parameters. In this example, we will use the SGD (Stochastic Gradient Descent) optimization method which requires to define the learning rate and momentum parameters. We also use a seed so that the experiments can be reproducible. We create an instance network of our architecture as well as the SGD optimizer. Finally, we define the loss that will be used during training. Here, we use the cross entropy loss, which is one of the most used in classification problems."
      ],
      "metadata": {
        "id": "DV5kBQuBhQ8u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wAdVg65_2tI"
      },
      "source": [
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 200\n",
        "\n",
        "random_seed = 34343\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "network = Net()\n",
        "\n",
        "print(network)\n",
        "\n",
        "total_params = sum(p.numel() for p in network.parameters())\n",
        "print('total parameters {}'.format(total_params))\n",
        "\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
        "                      momentum=momentum)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define the training procedure for our model. We need first to specify that we are using our network for training by calling the train() method of our model instance. Then, we define the operations that will be done for each training batch. First, we explicitly set the gradients to zero before starting to do backpropragation with the zero_grad method. Then, we pass our data through our network and we compute the loss. Then, we apply backpropagation with backward method. Finally, the weights of our network are updated by calling the method step from the optimizer. We also print the loss values every log_interval batches."
      ],
      "metadata": {
        "id": "QM3FiSx7ipNT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXrTs57CABSa"
      },
      "source": [
        "def train(epoch):\n",
        "    network.train()\n",
        "\n",
        "    correct = 0\n",
        "    loss_epoch = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "\n",
        "        loss_epoch += loss\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('  Training Epoch {}: [{}/{} ({:.0f}% done)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, \n",
        "                batch_idx * len(data), \n",
        "                len(train_loader.dataset), \n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "    accuracy_epoch = 100. * correct / len(train_loader.dataset)\n",
        "    loss_epoch = loss_epoch/ len(train_loader.dataset)\n",
        "\n",
        "    train_accuracy.append(accuracy_epoch.item())\n",
        "    train_losses.append(loss_epoch.item() /len(train_loader.dataset))\n",
        "\n",
        "    print('Train Set Epoch {} [Accuracy: {}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, correct, len(train_loader.dataset), \n",
        "                accuracy_epoch, loss_epoch))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the training procedure has been defined, we need to also define the test procedure. We need first to specify that we are using our network for testing by calling the eval() method of our model instance. We also initialize the loss to zero and the number of corrected predictions to zero. During testing we also need to specify that no gradients will be computed with the no_grad method from torch. Then, for each batch from the test subset, we pass the data through the network, we compute the loss by using the criterion function and update the test loss. For each test sample, we select the category with highest prediction score to know the predicted category. We compare the predicted categories with the ground truth labels to obtain the number of correct predictions. Finally, we print the loss in the test batch and the number of correct predictions."
      ],
      "metadata": {
        "id": "KV3AyjDolULk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFK6GHVlAFeZ"
      },
      "source": [
        "def test():\n",
        "    network.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = network(data)\n",
        "\n",
        "            test_loss += criterion(output, target)\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "\n",
        "        # test finished, lets store the results:\n",
        "        accuracy = 100. * correct / len(test_loader.dataset)\n",
        "        test_loss = test_loss/len(test_loader.dataset)\n",
        "\n",
        "        test_accuracy.append(accuracy.item())\n",
        "        test_losses.append(test_loss.item())\n",
        "\n",
        "        print('Test Set: [Accuracy: {}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            correct, len(test_loader.dataset), \n",
        "            accuracy, test_loss))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once defined both training and testing procedures, we proceed to train and evaluate our network. In our example, we will train our network for 3 epochs, which means that the whole dataset will be used 3 times to train the model. For each epoch, we first train the model and then we evaluate the model."
      ],
      "metadata": {
        "id": "Odc5BayowTFM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3dkJz2KALbc"
      },
      "source": [
        "n_epochs = 15\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "train_accuracy = []\n",
        "test_accuracy = []\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once our model has been trained, we will show the accuracy on the test set. We will also plot the training and testing losses."
      ],
      "metadata": {
        "id": "hSuDyIvOxE0J"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PhaIqJ2Agjp"
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=1)\n",
        "\n",
        "\n",
        "# plot losses\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(train_losses, color='blue')\n",
        "\n",
        "plt.title('Train Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('negative log likelihood loss')\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.title('Test Loss')\n",
        "plt.plot(test_losses, color='red')\n",
        "\n",
        "\n",
        "# plot accuracies\n",
        "plt.subplot(2,2,3)\n",
        "plt.plot(train_accuracy, color='blue')\n",
        "\n",
        "plt.title('Train Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "plt.title('Test Accuracy')\n",
        "plt.plot(test_accuracy, color='red')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will also show some predictions from 6 samples of one of the test batches. This way, we can have some qualitative results with the visualization of the input image and the category predicted."
      ],
      "metadata": {
        "id": "NDL2aAnSxq1S"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5QtS-UTBFFH"
      },
      "source": [
        "with torch.no_grad():\n",
        "  output = network(example_data)\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Prediction: {}\".format(\n",
        "    output.data.max(1, keepdim=True)[1][i].item()))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Y3CnnB6VsXYU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
