{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5_1_ImageClassification_BoF_SVM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN0NM3DSFbaIVQfvRgPuuAo"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<div style=\"width: 100%; clear: both;\">\n","<div style=\"float: left; width: 50%;\">\n","<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n","</div>\n","<div style=\"float: right; width: 50%;\">\n","<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M0.532 · Pattern Recognition</p>\n","<p style=\"margin: 0; text-align:right;\">Computational Engineering and Mathematics Master</p>\n","<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Computers, Multimedia and Telecommunications Department</p>\n","</div>\n","</div>\n","<div style=\"width:100%;\">&nbsp;</div>"],"metadata":{"id":"-gtnaVGOmCL3"}},{"cell_type":"markdown","source":["## Image Classification with Bag of Features (BoF) and Support Vector Machines (SVM)"],"metadata":{"id":"c7dLs4Ljm9Ea"}},{"cell_type":"markdown","source":["In this notebook, we will train an image classifier based on Bag of Features (BoF) and using SVMs. The BoF is a feature representation based on quantization. Using local features from a training dataset, these features are clustered and some representative features (the center data from each cluster) are used as reference. Once these representative features have been obtained, any image can be represented by a combination of these features. This process is done by extracting first the local features. Then, for each local feature, the nearest reference feature must be found. As a result, if we have M reference features (size of the codebook), the image will be represented as a vector f of dimension M, where each component $f_i$ will represent how many local features from the image have been assigned to the reference feature $i$. These vector representations of the images will be used to train an image classification system with Support Vector Machines."],"metadata":{"id":"KFwjPIHcnKqU"}},{"cell_type":"markdown","source":["Let's start by importing some data from our Google Drive account."],"metadata":{"id":"Q-EihEJrqOMV"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yM3DrWh2K7x1","executionInfo":{"status":"ok","timestamp":1637750007527,"user_tz":-60,"elapsed":3800,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"59d4d328-117d-4aed-fbe5-30c2631e132c"},"source":["from google.colab import drive\n"," \n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["We will be using the Caltech101 image dataset, which includes 101 categories. In the following code, we can see the names of the different categories included in Caltech101."],"metadata":{"id":"jkNnNlg-qWEj"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NiM95D5pLSlD","executionInfo":{"status":"ok","timestamp":1637748656900,"user_tz":-60,"elapsed":2971,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"35b20e54-183f-4598-dd4f-33ee674c0b84"},"source":["from glob import glob\n","from os.path import exists, isdir, basename, join, splitext\n","\n","datasetpath = '/content/drive/My Drive/Docència/Reconeixement de Patrons/Notebooks/Image Classification/101_ObjectCategories/'\n","\n","cat_paths = [files\n","              for files in glob(datasetpath + \"/*\")\n","              if isdir(files)]\n","cat_paths.sort()\n","cats = [basename(cat_path) for cat_path in cat_paths]\n","\n","print(cats)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['BACKGROUND_Google', 'Faces', 'Faces_easy', 'Leopards', 'Motorbikes', 'accordion', 'airplanes', 'anchor', 'ant', 'barrel', 'bass', 'beaver', 'binocular', 'bonsai', 'brain', 'brontosaurus', 'buddha', 'butterfly', 'camera', 'cannon', 'car_side', 'ceiling_fan', 'cellphone', 'chair', 'chandelier', 'cougar_body', 'cougar_face', 'crab', 'crayfish', 'crocodile', 'crocodile_head', 'cup', 'dalmatian', 'dollar_bill', 'dolphin', 'dragonfly', 'electric_guitar', 'elephant', 'emu', 'euphonium', 'ewer', 'ferry', 'flamingo', 'flamingo_head', 'garfield', 'gerenuk', 'gramophone', 'grand_piano', 'hawksbill', 'headphone', 'hedgehog', 'helicopter', 'ibis', 'inline_skate', 'joshua_tree', 'kangaroo', 'ketch', 'lamp', 'laptop', 'llama', 'lobster', 'lotus', 'mandolin', 'mayfly', 'menorah', 'metronome', 'minaret', 'nautilus', 'octopus', 'okapi', 'pagoda', 'panda', 'pigeon', 'pizza', 'platypus', 'pyramid', 'revolver', 'rhino', 'rooster', 'saxophone', 'schooner', 'scissors', 'scorpion', 'sea_horse', 'snoopy', 'soccer_ball', 'stapler', 'starfish', 'stegosaurus', 'stop_sign', 'strawberry', 'sunflower', 'tick', 'trilobite', 'umbrella', 'watch', 'water_lilly', 'wheelchair', 'wild_cat', 'windsor_chair', 'wrench', 'yin_yang']\n"]}]},{"cell_type":"markdown","source":["Let's focus on only two categories, e.g. airplanes and motorbikes, in order to make the problem easier and faster to be trained. Therefore, we will have a binary classification problem. We will need to classify the images as either an image containing an airplane or a motorbike."],"metadata":{"id":"4-wHZM59q-ru"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9oVT6_jzVg5f","executionInfo":{"status":"ok","timestamp":1637748660313,"user_tz":-60,"elapsed":247,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"5dd73d6b-21d3-4a06-9b95-5a6945a4c54a"},"source":["cats_used = ['airplanes', 'Motorbikes']\n","ncats = len(cats_used)\n","print(ncats)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n"]}]},{"cell_type":"markdown","source":["We install the OpenCV library in order to extract the local features from the images."],"metadata":{"id":"Fgym4IUHrcJu"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4w_ZG6KMXTTU","executionInfo":{"status":"ok","timestamp":1637748678464,"user_tz":-60,"elapsed":10331,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"c5c64151-d3d0-44e4-888d-4380aa5ba370"},"source":["!pip install opencv-contrib-python==4.4.0.44"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting opencv-contrib-python==4.4.0.44\n","  Downloading opencv_contrib_python-4.4.0.44-cp37-cp37m-manylinux2014_x86_64.whl (55.7 MB)\n","\u001b[K     |████████████████████████████████| 55.7 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==4.4.0.44) (1.19.5)\n","Installing collected packages: opencv-contrib-python\n","  Attempting uninstall: opencv-contrib-python\n","    Found existing installation: opencv-contrib-python 4.1.2.30\n","    Uninstalling opencv-contrib-python-4.1.2.30:\n","      Successfully uninstalled opencv-contrib-python-4.1.2.30\n","Successfully installed opencv-contrib-python-4.4.0.44\n"]}]},{"cell_type":"markdown","source":["We import the openCV cv2 module which will allows us to use some image transformations as well as extract the local features. Let's define a function extractSIFT, which computes the extract and computed the SIFT descriptors for a set of input_files given."],"metadata":{"id":"yLD1kndHrt1Z"}},{"cell_type":"code","metadata":{"id":"RG-Tpp6yFwJo"},"source":["import cv2 as cv\n","\n","def extractSIFT(input_files):\n","    all_features_dict = {}\n","    feature_extractor = cv.SIFT.create()\n","    for i, fname in enumerate(input_files):\n","        rgb = cv.cvtColor(cv.imread(fname), cv.COLOR_BGR2RGB)\n","        gray = cv.cvtColor(rgb, cv.COLOR_RGB2GRAY)\n","        kp, desc = feature_extractor.detectAndCompute(gray, None)\n","        all_features_dict[fname] = desc\n","    return all_features_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's also define another function that returns all the image filenames from a given path."],"metadata":{"id":"2X0nSxWWsfkI"}},{"cell_type":"code","metadata":{"id":"LO9S2utgGI2C"},"source":["EXTENSIONS = [\".jpg\", \".bmp\", \".png\", \".pgm\", \".tif\", \".tiff\"]\n","\n","def get_imgfiles(path):\n","    all_files = []\n","    all_files.extend([join(path, basename(fname))\n","                    for fname in glob(path + \"/*\")\n","                    if splitext(fname)[-1].lower() in EXTENSIONS])\n","    return all_files"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, we use both previous functions to compute the SIFT feature vectors from all the images from the two categories considered above."],"metadata":{"id":"d6azCTaBsn2T"}},{"cell_type":"code","metadata":{"id":"eYpAu5eyFIrB"},"source":["all_files = []\n","all_files_labels = {}\n","all_features = {}\n","cat_label = {}\n","\n","for cat, label in zip(cats_used, range(ncats)):\n","    cat_path = join(datasetpath, cat)\n","    cat_files = get_imgfiles(cat_path)\n","    cat_features = extractSIFT(cat_files)\n","    all_files = all_files + cat_files\n","    all_features.update(cat_features)\n","    cat_label[cat] = label\n","    for i in cat_files:\n","        all_files_labels[i] = label"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we have explained at the beginning of the notebook, we need to cluster the SIFT local features to generate a codebook, which will be used later to represent any image as a feature vector. In order to generate this codebook, we can use the BOWKMeansTrainer function from OpenCV. We need to specify the size of our codebook or dictionary, e.g. 100. We add all the SIFT features computed previously and we apply the function cluster to generate the codeworks or representative features of our dictionary."],"metadata":{"id":"crZxo5SGtCC3"}},{"cell_type":"code","metadata":{"id":"Vqey6YH31lZc"},"source":["dictionarySize = 100\n","BOW = cv.BOWKMeansTrainer(dictionarySize)\n","\n","for feat in all_features:\n","    BOW.add(all_features[feat])\n","dictionary = BOW.cluster()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can check the shape of our dictionary. The first dimension (100) correspond to the size of the dictionary. The second dimension (128) correspond to the number of components that any SIFT feature vector has."],"metadata":{"id":"gXKkaM8GuDak"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HPWTNSHXBC0k","executionInfo":{"status":"ok","timestamp":1637755916083,"user_tz":-60,"elapsed":234,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"5a307efc-fcbc-4a04-f3c0-9060b9f4eb7f"},"source":["print(dictionary.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(100, 128)\n"]}]},{"cell_type":"markdown","source":["Let's also check the shape of the features extracted of a given image. The first dimension (203) correspond to the number of keypoints or interesting points detected on the given image. The second dimension (128) corresponds to the number of components that any SIFT feature vector has. The number of keypoints will vary from one image to another. We will use the dictionary created before to have a fixed-size representation of any image."],"metadata":{"id":"2je-brTdud60"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NL0h6baQHQHC","executionInfo":{"status":"ok","timestamp":1637755918237,"user_tz":-60,"elapsed":245,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"34e00b1d-241b-41c2-c0ed-16e9262eb88d"},"source":["print(all_features['/content/drive/My Drive/Docència/Reconeixement de Patrons/Notebooks/Image Classification/101_ObjectCategories/Motorbikes/image_0734.jpg'].shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(203, 128)\n"]}]},{"cell_type":"markdown","source":["In order to generate a fixed-size representation for each image, what we need to do is assign each local feature to the nearest codework from our dictionary. To find the nearest codework we will use the BFMatcher, which checks the distance of a given feature to all codeworks features from our dictionary by Brute Force (BF). If we want to have a faster system, other approaches such as indexed trees can be used to find the nearest codework in a more efficient way, e.g. FlannBasedMatcher.\n","\n","In the following code, we use the BFMatcher to find the nearest codework from our dictionary for each local feature (desc_query) from each image. Once we have found the nearest codework for all features of a given image, we compute the histogram in order to have a normalized representation. This is done because the number of features extracted from a given image varies from one to another.\n","\n","We store the new feature representation in a variable named X and the corresponding label (referring to the image category) in a variable named y, which will be used later to train the image classification model."],"metadata":{"id":"pU634nldvTdO"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXV3NpquixWk","executionInfo":{"status":"ok","timestamp":1637755922230,"user_tz":-60,"elapsed":2173,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"5333cb9f-d5d5-4e9a-9e91-92dbba5ff3bc"},"source":["from numpy import histogram\n","import numpy as np\n","\n","matcher = cv.BFMatcher(normType=cv.NORM_L2)\n","all_features_BOW = {}\n","\n","X = np.empty((len(all_files),dictionarySize))\n","y = np.empty((len(all_files),))\n","\n","count = 0\n","for filename in all_files:\n","    desc_query = all_features[filename]\n","    matches = matcher.match(desc_query,dictionary)\n","    train_idxs = []\n","    for j in range(len(matches)):\n","      train_idxs.append(matches[j].trainIdx)\n","    hist, bin_edges = histogram(train_idxs, bins=range(dictionarySize+1),normed=True)\n","    all_features_BOW[filename] = hist\n","    X[count,:] = hist\n","    y[count] = all_files_labels[filename]\n","    count = count + 1\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n"]}]},{"cell_type":"markdown","source":["We already have a fixed-size feature representation based on SIFT descriptors for each image in our dataset. Therefore, we can train a classifier based on them. For that, we will use Support Vector Machines (SVM) from the Scikit-learn library. We will also use the function train_test_split from this library to split the data into two subsets, one for training and another one for testing. Then, we train the model by using the method fit from the function SVC. Once the model has been trained, we obtain the classification accuracy by using the function score."],"metadata":{"id":"BzYw3SzKxdVc"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFJMhhCdt-jb","executionInfo":{"status":"ok","timestamp":1637755925620,"user_tz":-60,"elapsed":280,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"3039f0ac-a0e3-4a4d-bf69-64b74558958a"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n","\n","clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n","clf.score(X_test, y_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9047619047619048"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["Instead of just doing a single split into training and test subsets, we can also define multiple splits by using the function StratifiedKFold. "],"metadata":{"id":"DtNiemXpy7w3"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BzRYyNqj1xik","executionInfo":{"status":"ok","timestamp":1637755929294,"user_tz":-60,"elapsed":685,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"c647bfb0-70ee-48ec-d6a7-d42cbfe8b100"},"source":["from sklearn.model_selection import StratifiedKFold, KFold\n","\n","scores = []\n","skf = StratifiedKFold(n_splits=5)\n","for train, test in skf.split(X, y):\n","  clf = svm.SVC(kernel='linear', C=1).fit(X[train], y[train])\n","  score = clf.score(X[test], y[test])\n","  scores.append(score)\n","\n","print(scores)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8385093167701864, 0.9503105590062112, 0.9285714285714286, 0.9003115264797508, 0.9345794392523364]\n"]}]}]}