{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8_1_ActionRecognition.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO7uBmWiZT+E+VXyfrZN3tw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0c1e722b75574cc28019b2bd5d77b5ee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bec0a49f92b4412f889a5a52bad6ed67","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_00cc1045db5047839daa25b4b6742076","IPY_MODEL_807a1db16ca04292a2fa339f7677c86b","IPY_MODEL_7de426f3d2624796b469751a03bb353e"]}},"bec0a49f92b4412f889a5a52bad6ed67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"00cc1045db5047839daa25b4b6742076":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cecacc679d3c481e8889df04137d65b0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3357f11b12254879be967c51f15325e7"}},"807a1db16ca04292a2fa339f7677c86b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_42fc270997b644bc9d614fe2fbb83b54","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":277138115,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":277138115,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f4658f16633648cfa84372c555e9ea12"}},"7de426f3d2624796b469751a03bb353e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_abec6f35f6fd4eaea8f0124fc99e1a3c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 264M/264M [00:04&lt;00:00, 69.6MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_34d236e072734959993e607287ae8062"}},"cecacc679d3c481e8889df04137d65b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3357f11b12254879be967c51f15325e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"42fc270997b644bc9d614fe2fbb83b54":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f4658f16633648cfa84372c555e9ea12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"abec6f35f6fd4eaea8f0124fc99e1a3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"34d236e072734959993e607287ae8062":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","source":["<div style=\"width: 100%; clear: both;\">\n","<div style=\"float: left; width: 50%;\">\n","<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n","</div>\n","<div style=\"float: right; width: 50%;\">\n","<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M0.532 · Pattern Recognition</p>\n","<p style=\"margin: 0; text-align:right;\">Computational Engineering and Mathematics Master</p>\n","<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Computers, Multimedia and Telecommunications Department</p>\n","</div>\n","</div>\n","<div style=\"width:100%;\">&nbsp;</div>"],"metadata":{"id":"epZ5eac1p1LM"}},{"cell_type":"markdown","metadata":{"id":"H7ngd4brbQeF"},"source":["In this notebook, we will use [PytorchVideo](https://pytorchvideo.org/), a deep learning library for video understanding research. More specifically, we will deal with the action recognition task. This task consists on recognizing which action is being done in a video.\n","\n","\n","\n","\n"]},{"cell_type":"code","source":[""],"metadata":{"id":"MGWSRDDNp2WE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["First of all, we need to install the PytorchVideo library."],"metadata":{"id":"fWI1C_3cDk9H"}},{"cell_type":"code","source":["pip install pytorchvideo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wb5F1YMgK0XI","executionInfo":{"status":"ok","timestamp":1639858276565,"user_tz":-60,"elapsed":11260,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"3ac7a9d0-da04-49d8-ceba-e543126ccb75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorchvideo\n","  Downloading pytorchvideo-0.1.3.tar.gz (128 kB)\n","\u001b[K     |████████████████████████████████| 128 kB 7.6 MB/s \n","\u001b[?25hCollecting fvcore\n","  Downloading fvcore-0.1.5.post20211023.tar.gz (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 7.1 MB/s \n","\u001b[?25hCollecting av\n","  Downloading av-8.0.3-cp37-cp37m-manylinux2010_x86_64.whl (37.2 MB)\n","\u001b[K     |████████████████████████████████| 37.2 MB 1.2 MB/s \n","\u001b[?25hCollecting parameterized\n","  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\n","Collecting iopath\n","  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (1.19.5)\n","Collecting yacs>=0.1.6\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 43.0 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (4.62.3)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (1.1.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (7.1.2)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (0.8.9)\n","Collecting portalocker\n","  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n","Building wheels for collected packages: pytorchvideo, fvcore\n","  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.3-py3-none-any.whl size=183829 sha256=e749422ab8af6681b2f002aaca9be3f8328c95d807c3082ece93be2786993af4\n","  Stored in directory: /root/.cache/pip/wheels/d4/a7/4c/bada8b1065ae9befac2da6d7f6648cd6718681eb7901ca226d\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20211023-py3-none-any.whl size=60947 sha256=daadc516a3132015c65b888ae3f073287ded849009746c95ac4c4045f6629efa\n","  Stored in directory: /root/.cache/pip/wheels/16/98/fc/252d62cab6263c719120e06b28f3378af59b52ce7a20e81852\n","Successfully built pytorchvideo fvcore\n","Installing collected packages: pyyaml, portalocker, yacs, iopath, parameterized, fvcore, av, pytorchvideo\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed av-8.0.3 fvcore-0.1.5.post20211023 iopath-0.1.9 parameterized-0.8.1 portalocker-2.3.2 pytorchvideo-0.1.3 pyyaml-6.0 yacs-0.1.8\n"]}]},{"cell_type":"markdown","source":["Then, we import some helper functions included in PytorchVideo to make some data transformations."],"metadata":{"id":"_3gYruyXDrLm"}},{"cell_type":"code","metadata":{"id":"olPGgpddbNy1"},"source":["import torch\n","import json\n","from torchvision.transforms import Compose, Lambda\n","from torchvision.transforms._transforms_video import (\n","    CenterCropVideo,\n","    NormalizeVideo,\n",")\n","from pytorchvideo.data.encoded_video import EncodedVideo\n","from pytorchvideo.transforms import (\n","    ApplyTransformToKey,\n","    ShortSideScale,\n","    UniformTemporalSubsample,\n","    UniformCropVideo\n",")\n","from typing import Dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We download a video that we will use to recognize which action is taking place."],"metadata":{"id":"pHl5i94xD7v8"}},{"cell_type":"code","source":["!wget https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ExNtXgyLNIY","executionInfo":{"status":"ok","timestamp":1639858966912,"user_tz":-60,"elapsed":640,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"1d58c41c-2034-4a6e-b8da-6201fd4e12f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-12-18 20:22:46--  https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 549197 (536K) [video/mp4]\n","Saving to: ‘archery.mp4’\n","\n","archery.mp4         100%[===================>] 536.33K  --.-KB/s    in 0.1s    \n","\n","2021-12-18 20:22:46 (4.61 MB/s) - ‘archery.mp4’ saved [549197/549197]\n","\n"]}]},{"cell_type":"markdown","source":["We specify that we will use the SlowFast pretrained model for action recognition and that we will use it for inference (eval method)."],"metadata":{"id":"N-kpi7mxEJah"}},{"cell_type":"code","source":["# Device on which to run the model\n","# Set to cuda to load on GPU\n","device = \"cpu\"\n","\n","# Pick a pretrained model and load the pretrained weights\n","model_name = \"slowfast_r50\"\n","model = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)\n","\n","# Set to eval mode and move to desired device\n","model = model.to(device)\n","model = model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103,"referenced_widgets":["0c1e722b75574cc28019b2bd5d77b5ee","bec0a49f92b4412f889a5a52bad6ed67","00cc1045db5047839daa25b4b6742076","807a1db16ca04292a2fa339f7677c86b","7de426f3d2624796b469751a03bb353e","cecacc679d3c481e8889df04137d65b0","3357f11b12254879be967c51f15325e7","42fc270997b644bc9d614fe2fbb83b54","f4658f16633648cfa84372c555e9ea12","abec6f35f6fd4eaea8f0124fc99e1a3c","34d236e072734959993e607287ae8062"]},"id":"GIDl_ygQLVfH","executionInfo":{"status":"ok","timestamp":1639858410373,"user_tz":-60,"elapsed":7906,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"253ae2d7-3a78-4d79-bb75-a2d649a1e923"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/facebookresearch/pytorchvideo/archive/main.zip\" to /root/.cache/torch/hub/main.zip\n","Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST_8x8_R50.pyth\" to /root/.cache/torch/hub/checkpoints/SLOWFAST_8x8_R50.pyth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c1e722b75574cc28019b2bd5d77b5ee","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/264M [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"markdown","source":["We download the action category names from Kinetcs Dataset, which is the dataset where the model has been pretrained."],"metadata":{"id":"IvBvlxU6Erey"}},{"cell_type":"code","source":["!wget https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0v0s4HqQLcRa","executionInfo":{"status":"ok","timestamp":1639858427822,"user_tz":-60,"elapsed":639,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"8f4a2680-056c-4d75-e0d0-b5ccc603b366"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-12-18 20:13:47--  https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10326 (10K) [text/plain]\n","Saving to: ‘kinetics_classnames.json’\n","\n","\rkinetics_classnames   0%[                    ]       0  --.-KB/s               \rkinetics_classnames 100%[===================>]  10.08K  --.-KB/s    in 0s      \n","\n","2021-12-18 20:13:47 (55.3 MB/s) - ‘kinetics_classnames.json’ saved [10326/10326]\n","\n"]}]},{"cell_type":"markdown","source":["Then, we create an id to label name mapping."],"metadata":{"id":"RNKg815GxJbR"}},{"cell_type":"code","source":["with open(\"kinetics_classnames.json\", \"r\") as f:\n","    kinetics_classnames = json.load(f)\n","\n","# Create an id to label name mapping\n","kinetics_id_to_classname = {}\n","for k, v in kinetics_classnames.items():\n","    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"],"metadata":{"id":"b9nWUcN8Lg8l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we define a serie of transforms. Videos are first uniformly sampled to get a set of frames. Then, the frames are normalized, rescaled and center cropped. Finally, the frames are converted to a list of tensors."],"metadata":{"id":"Ow2mp2QIxb7U"}},{"cell_type":"code","source":["####################\n","# SlowFast transform\n","####################\n","\n","side_size = 256\n","mean = [0.45, 0.45, 0.45]\n","std = [0.225, 0.225, 0.225]\n","crop_size = 256\n","num_frames = 32\n","sampling_rate = 2\n","frames_per_second = 30\n","alpha = 4\n","\n","class PackPathway(torch.nn.Module):\n","    \"\"\"\n","    Transform for converting video frames as a list of tensors.\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, frames: torch.Tensor):\n","        fast_pathway = frames\n","        # Perform temporal sampling from the fast pathway.\n","        slow_pathway = torch.index_select(\n","            frames,\n","            1,\n","            torch.linspace(\n","                0, frames.shape[1] - 1, frames.shape[1] // alpha\n","            ).long(),\n","        )\n","        frame_list = [slow_pathway, fast_pathway]\n","        return frame_list\n","\n","transform =  ApplyTransformToKey(\n","    key=\"video\",\n","    transform=Compose(\n","        [\n","            UniformTemporalSubsample(num_frames),\n","            Lambda(lambda x: x/255.0),\n","            NormalizeVideo(mean, std),\n","            ShortSideScale(\n","                size=side_size\n","            ),\n","            CenterCropVideo(crop_size),\n","            PackPathway()\n","        ]\n","    ),\n",")\n","\n","# The duration of the input clip is also specific to the model.\n","clip_duration = (num_frames * sampling_rate)/frames_per_second"],"metadata":{"id":"oHs7j5iML2KT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We select the video that we want to use and we apply the previous transforms to normalize it."],"metadata":{"id":"7aDUWLb3ypo3"}},{"cell_type":"code","source":["# Load the example video\n","video_path = \"archery.mp4\"\n","\n","# Select the duration of the clip to load by specifying the start and end duration\n","# The start_sec should correspond to where the action occurs in the video\n","start_sec = 0\n","end_sec = start_sec + clip_duration\n","\n","# Initialize an EncodedVideo helper class\n","video = EncodedVideo.from_path(video_path)\n","\n","# Load the desired clip\n","video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n","\n","# Apply a transform to normalize the video input\n","video_data = transform(video_data)\n","\n","# Move the inputs to the desired device\n","inputs = video_data[\"video\"]\n","inputs = [i.to(device)[None, ...] for i in inputs]"],"metadata":{"id":"Xqs8J45VMAvG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The list of tensors returned by the previous code (inputs) is passed through the model to get the predictions (preds)."],"metadata":{"id":"NmXu7Lwhy8Bm"}},{"cell_type":"code","source":["# Pass the input clip through the model\n","preds = model(inputs)"],"metadata":{"id":"k9RY1BpRMjKI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We get the top-5 predicted action categories with the corresponding confidence score values."],"metadata":{"id":"7FWD8GTRzPVL"}},{"cell_type":"code","source":["# Get the predicted classes\n","post_act = torch.nn.Softmax(dim=1)\n","preds_max = post_act(preds)\n","pred_classes = preds_max.topk(k=5).indices\n","\n","# Map the predicted classes to the label names\n","pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes[0]]\n","print(\"Predicted labels: %s\" % \", \".join(pred_class_names))\n","print(preds_max.topk(k=5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umQvXhq1Mrt-","executionInfo":{"status":"ok","timestamp":1639859129265,"user_tz":-60,"elapsed":366,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"b0f1df16-310d-4283-fa55-b2da788287d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted labels: archery, throwing axe, playing paintball, disc golfing, riding or walking with horse\n","torch.return_types.topk(\n","values=tensor([[1.0000e+00, 1.6928e-19, 5.2984e-22, 2.7325e-23, 2.4635e-23]],\n","       grad_fn=<TopkBackward0>),\n","indices=tensor([[  5, 356, 240,  92, 273]]))\n"]}]},{"cell_type":"markdown","source":["Let's try it with another input video."],"metadata":{"id":"o7XsZu0Tz1Y9"}},{"cell_type":"code","source":["!wget https://www.bogotobogo.com/python/OpenCV_Python/images/mean_shift_tracking/slow_traffic_small.mp4"],"metadata":{"id":"kI_ydLHiOLlo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the example video\n","video_path = \"slow_traffic_small.mp4\"\n","\n","# Select the duration of the clip to load by specifying the start and end duration\n","# The start_sec should correspond to where the action occurs in the video\n","start_sec = 0\n","end_sec = start_sec + clip_duration\n","\n","# Initialize an EncodedVideo helper class\n","video = EncodedVideo.from_path(video_path)\n","\n","# Load the desired clip\n","video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n","\n","# Apply a transform to normalize the video input\n","video_data = transform(video_data)\n","\n","# Move the inputs to the desired device\n","inputs = video_data[\"video\"]\n","inputs = [i.to(device)[None, ...] for i in inputs]"],"metadata":{"id":"brWl4GFuOSC2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pass the input clip through the model\n","preds = model(inputs)"],"metadata":{"id":"u0suMes4OYL2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the predicted classes\n","post_act = torch.nn.Softmax(dim=1)\n","preds_max = post_act(preds)\n","pred_classes = preds_max.topk(k=5).indices\n","\n","# Map the predicted classes to the label names\n","pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes[0]]\n","print(\"Predicted labels: %s\" % \", \".join(pred_class_names))\n","print(preds_max.topk(k=5))"],"metadata":{"id":"1aGnhjvWObH6","executionInfo":{"status":"ok","timestamp":1639859232641,"user_tz":-60,"elapsed":372,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"ec66d7c6-36a3-4f4e-c306-f69a49c6de1a","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted labels: driving car, motorcycling, pushing car, texting, changing wheel\n","torch.return_types.topk(\n","values=tensor([[9.9150e-01, 7.3789e-03, 9.4220e-04, 1.0740e-04, 4.3958e-05]],\n","       grad_fn=<TopkBackward0>),\n","indices=tensor([[103, 199, 261, 355,  53]]))\n"]}]}]}