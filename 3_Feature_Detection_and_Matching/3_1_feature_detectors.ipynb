{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.1_feature_detectors.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"width: 100%; clear: both;\">\n",
        "<div style=\"float: left; width: 50%;\">\n",
        "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
        "</div>\n",
        "<div style=\"float: right; width: 50%;\">\n",
        "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M0.532 Â· Pattern Recognition</p>\n",
        "<p style=\"margin: 0; text-align:right;\">Computational Engineering and Mathematics Master</p>\n",
        "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Computers, Multimedia and Telecommunications Department</p>\n",
        "</div>\n",
        "</div>\n",
        "<div style=\"width:100%;\">&nbsp;</div>"
      ],
      "metadata": {
        "id": "lf9fpMypvVxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Detectors\n"
      ],
      "metadata": {
        "id": "n8RmGd-HDcvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this notebook we need to do the install of the contrib modules from OpenCV. Contrib contains patended algorithms and others under development. SIFT and ORB algorithms are in contrib module"
      ],
      "metadata": {
        "id": "fpxH6uC1Lvmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-contrib-python==4.4.0.44"
      ],
      "metadata": {
        "id": "QhvRxtx1Ltkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWz6Urycfq6C"
      },
      "source": [
        "# import OpenCV library\n",
        "import cv2\n",
        "\n",
        "# we will use the following import to display images in colab:\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNP3NtFBfggJ"
      },
      "source": [
        "!wget https://github.com/opencv/opencv/blob/master/samples/data/rubberwhale1.png?raw=true -O rubberwhale1.png\n",
        "\n",
        "# read image\n",
        "img = cv2.imread('rubberwhale1.png', cv2.IMREAD_COLOR)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSGvqc_6cxsV"
      },
      "source": [
        "cv2_imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient computation\n",
        "\n",
        "Feature detectors are interested in areas where high gradients are present: lets compute the gradient (find the areas with high frequencies) with the sobel operator that we saw in the previous chapter:"
      ],
      "metadata": {
        "id": "Z9-5pX9yEYiy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjhlhY0xFNby"
      },
      "source": [
        "# lets compute the filter with horizontal and vertical:\n",
        "\n",
        "# sobel x direction parameters (sx)\n",
        "s_xorder = 1\n",
        "s_yorder = 1\n",
        "s_ksize = 3\n",
        "\n",
        "img_sobel = cv2.Sobel(img,cv2.CV_64F,s_xorder,s_yorder,ksize=s_ksize) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la3uXeFNFNbz"
      },
      "source": [
        "# lets display the edges:\n",
        "cv2_imshow(img_sobel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the borders of objects are the areas with higher gradients: those areas should be the ones containing more feature descriptors!"
      ],
      "metadata": {
        "id": "JgG3n349FZXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Harris detector\n",
        "\n",
        "OpenCV has the [cornerHarris](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#gac1fc3598018010880e370e2f709b4345) function to compute the Harris corner detector\n"
      ],
      "metadata": {
        "id": "zN48CfgRF7j_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# the input of harris detector is a gray image:\n",
        "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n"
      ],
      "metadata": {
        "id": "R19I-HXoEG3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cornerHarris function:\n",
        "\n",
        "```\n",
        "\n",
        " cv.cornerHarris(\tsrc, blockSize, ksize, k[, dst[, borderType]]\t) ->\tdst\n",
        "\n",
        "Parameters:\n",
        "  src single-channel 8-bit or floating-point image.\n",
        "  blockSize\tNeighborhood size \n",
        "  ksize\tAperture parameter for the Sobel operator.\n",
        "  k\tHarris detector free parameter.\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "gAIfcGlHGo5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blockSize = 2\n",
        "ksize = 3\n",
        "k = 0.04\n",
        "dst = cv2.cornerHarris(gray,blockSize, ksize, k)\n"
      ],
      "metadata": {
        "id": "97LfKTAXH_cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the cornerHarris is a value for each pixel. Higher values indicate higher probability of having a corner / interest point in that location. Lets see the output of the harris detector:\n"
      ],
      "metadata": {
        "id": "-2YLGNojIxle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing\n",
        "dst_norm = np.empty(dst.shape, dtype=np.float32)\n",
        "cv2.normalize(dst, dst_norm, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
        "dst_norm_scaled = cv2.convertScaleAbs(dst_norm)"
      ],
      "metadata": {
        "id": "FNPEr8kHINLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(dst_norm_scaled)"
      ],
      "metadata": {
        "id": "pDPFrYzyI6Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is similar at the sobel filter but edges where both gradients are high are more prominent (white pixels)"
      ],
      "metadata": {
        "id": "Io4RlUcZJbtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can find the harris corner points thresholding the output image: "
      ],
      "metadata": {
        "id": "QJXOdpMEJyFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Threshold harris (optimal depends on each image)\n",
        "threshold_harris = 0.02\n",
        "\n",
        "# lets make a copy of the input image to plot the results\n",
        "img_harris = img.copy()\n",
        "\n",
        "# pixels higher than the threshold provided are changed to red color:\n",
        "img_harris[dst>threshold_harris*dst.max()]=[0,0,255]"
      ],
      "metadata": {
        "id": "TNKjvWm8Jxcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(img_harris)"
      ],
      "metadata": {
        "id": "r3Ht80o8GI0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try with different thresholds and determine which is better for the example image "
      ],
      "metadata": {
        "id": "5rjoiAYqKVzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More advanced detectors: SIFT, SURF and ORB\n",
        "\n",
        "We  will execute the different algorithms in parallel since all the algorithms are in the contib modules and the API is equivalent for both of them\n",
        "\n",
        "* [SIFT](https://docs.opencv.org/4.5.4/d7/d60/classcv_1_1SIFT.html):  Scale Invariant Feature Transform\n",
        "\n",
        "* [ORB](https://docs.opencv.org/4.5.4/db/d95/classcv_1_1ORB.html):  oriented BRIEF"
      ],
      "metadata": {
        "id": "HyEN_h2yLC_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We can initialize the algorithms (lets use default parameters):"
      ],
      "metadata": {
        "id": "wTeO_5_-Ms61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "orb = cv2.ORB_create()"
      ],
      "metadata": {
        "id": "9jSIYqoYGOwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the detector in our image:"
      ],
      "metadata": {
        "id": "g3mgXRr0VkQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keypoints_sift = sift.detect(img)\n",
        "keypoints_orb = orb.detect(img)\n"
      ],
      "metadata": {
        "id": "asm7ZLS0VkiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And plot the results:"
      ],
      "metadata": {
        "id": "xax1GKOgN6P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_sift = np.empty((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
        "img_sift = cv2.drawKeypoints(img, keypoints_sift, img_sift)\n",
        "\n",
        "img_orb = np.empty((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
        "img_orb = cv2.drawKeypoints(img, keypoints_orb, img_orb)"
      ],
      "metadata": {
        "id": "SzPXvN6FNZ6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(cv2.hconcat([img_sift, img_orb]))"
      ],
      "metadata": {
        "id": "MYPD2DdENlj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SIFT and ORB detectors obtain different keypoints than harris. Can you change the parameters of SIFT and ORB and see how the keypoints detected change?"
      ],
      "metadata": {
        "id": "UMIGnWVmWTWD"
      }
    }
  ]
}