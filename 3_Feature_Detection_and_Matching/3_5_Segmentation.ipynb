{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.5_Segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GrtREU9gLfBB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"width: 100%; clear: both;\">\n",
        "<div style=\"float: left; width: 50%;\">\n",
        "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
        "</div>\n",
        "<div style=\"float: right; width: 50%;\">\n",
        "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M0.532 Â· Pattern Recognition</p>\n",
        "<p style=\"margin: 0; text-align:right;\">Computational Engineering and Mathematics Master</p>\n",
        "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Computers, Multimedia and Telecommunications Department</p>\n",
        "</div>\n",
        "</div>\n",
        "<div style=\"width:100%;\">&nbsp;</div>"
      ],
      "metadata": {
        "id": "lf9fpMypvVxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segmentation"
      ],
      "metadata": {
        "id": "SAq5emDRB8ty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import libraries"
      ],
      "metadata": {
        "id": "FzaYIsTWDKOM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DZbHOwYBgUU"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-contrib-python==4.4.0.44"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import OpenCV library\n",
        "import cv2\n",
        "\n",
        "# we will use the following import to display images in colab:\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "bdQDue_eDMst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load images"
      ],
      "metadata": {
        "id": "8pagOahcC3LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/opencv/opencv/blob/master/samples/data/rubberwhale1.png?raw=true -O rubberwhale1.png\n",
        "!wget https://github.com/opencv/opencv/blob/master/samples/data/squirrel_cls.jpg?raw=true -O squirrel_cls.jpg\n",
        "!wget https://github.com/opencv/opencv/blob/master/samples/data/aero1.jpg?raw=true -O aero1.jpg\n",
        "\n",
        "# read image\n",
        "img1 = cv2.imread('rubberwhale1.png', cv2.IMREAD_COLOR)\n",
        "img2 = cv2.imread('squirrel_cls.jpg', cv2.IMREAD_COLOR)\n",
        "img3 = cv2.imread('aero1.jpg', cv2.IMREAD_COLOR)\n",
        "\n",
        "# resize images to ease the display\n",
        "(height, width, channels) = img1.shape\n",
        "img2 = cv2.resize(img2, dsize=(width, height))\n",
        "img3 = cv2.resize(img3, dsize=(width, height))\n",
        "\n",
        "# convert image to grayscale\n",
        "#img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n"
      ],
      "metadata": {
        "id": "XLJB5rxSC7gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the images"
      ],
      "metadata": {
        "id": "Y-ryBDBvHF8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(cv2.hconcat([img1, img2, img3]))"
      ],
      "metadata": {
        "id": "y2hqJ-AUDOJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GrabCut"
      ],
      "metadata": {
        "id": "3HUpfC7fHRZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example from: https://docs.opencv.org/3.4/d8/d83/tutorial_py_grabcut.html"
      ],
      "metadata": {
        "id": "TrWIhVmDJCmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets work with the squirrel image:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yErfzlLIJLvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_grabcut = img2.copy()\n",
        "cv2_imshow(img_grabcut)"
      ],
      "metadata": {
        "id": "kVxnK9ydJIJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to draw a bounding box around the squirrel:"
      ],
      "metadata": {
        "id": "OtEgJlDAJOVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# user input (position of the bounding box: initial_x, initial_y, final_x, final_y)\n",
        "rect = (140,100,400,330)\n",
        "\n",
        "# lets plot the bounding box:\n",
        "color = (255,0,0)\n",
        "thickness = 3\n",
        "img_rect = img_grabcut.copy()\n",
        "img_rect= cv2.rectangle(img_rect,(rect[0], rect[1]),(rect[2], rect[3]), color, thickness)\n",
        "\n",
        "cv2_imshow(img_rect)"
      ],
      "metadata": {
        "id": "k8FFiMwuJVB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mask = np.zeros(img_grabcut.shape[:2],np.uint8)\n",
        "bgdModel = np.zeros((1,65),np.float64)\n",
        "fgdModel = np.zeros((1,65),np.float64)\n",
        "\n",
        "cv2.grabCut(img_grabcut,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)\n",
        "mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
        "result_grabcut = img_grabcut*mask2[:,:,np.newaxis]\n"
      ],
      "metadata": {
        "id": "zYYuA0VJB8eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(result_grabcut)"
      ],
      "metadata": {
        "id": "DCRabceyB8h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agglomerative Clustering\n"
      ],
      "metadata": {
        "id": "GrtREU9gLfBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from: https://scikit-learn.org/0.15/auto_examples/cluster/plot_lena_ward_segmentation.html"
      ],
      "metadata": {
        "id": "A1p_72qLuI0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import time as time\n",
        "import numpy as np\n",
        "# import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.image import grid_to_graph\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# lets define a function to do the agglomerative clustering of an image:\n",
        "\n",
        "def agglomerative_clustering_image (img, n_clusters):\n",
        "\n",
        "    # img_reg_merg = img2.copy()\n",
        "    # input image is grayscale:\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    ###############################################################################\n",
        "    # Generate data\n",
        "    X = np.reshape(img, (-1, 1))\n",
        "\n",
        "    ###############################################################################\n",
        "    # Define the structure A of the data. Pixels connected to their neighbors.\n",
        "    connectivity = grid_to_graph(*img.shape)\n",
        "\n",
        "    ###############################################################################\n",
        "    # Compute clustering\n",
        "    print(\"Compute structured hierarchical clustering...\")\n",
        "    ward = AgglomerativeClustering(n_clusters=n_clusters,\n",
        "            linkage='ward', connectivity=connectivity).fit(X)\n",
        "    label = np.reshape(ward.labels_, img.shape)\n",
        "    print(\"Number of pixels: \", label.size)\n",
        "    print(\"Number of clusters: \", np.unique(label).size)\n",
        "\n",
        "    return label\n",
        "\n",
        "\n",
        "def plot_aggl_clustering_contours(img, n_clusters, labels):\n",
        "\n",
        "  ###############################################################################\n",
        "  # Plot the results on an image\n",
        "\n",
        "  plt.figure(figsize=(5, 5))\n",
        "  plt.imshow(img, cmap=plt.cm.gray)\n",
        "\n",
        "  cmap = plt.cm.get_cmap(\"Spectral\")\n",
        "\n",
        "  for l in range(n_clusters):\n",
        "      plt.contour(labels == l,\n",
        "                  colors=[cmap(l / float(n_clusters)), ])\n",
        "  plt.xticks(())\n",
        "  plt.yticks(())\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "r7eyqECluI7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 45  # number of regions to generate\n",
        "\n",
        "\n",
        "labels_img1 = agglomerative_clustering_image (img1, n_clusters)\n",
        "labels_img2 = agglomerative_clustering_image (img2, n_clusters)\n",
        "labels_img3 = agglomerative_clustering_image (img3, n_clusters)"
      ],
      "metadata": {
        "id": "_sO3SNxYy4RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_aggl_clustering_contours(img1, n_clusters, labels_img1)"
      ],
      "metadata": {
        "id": "x6LAN9fWyxgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_aggl_clustering_contours(img2, n_clusters, labels_img2)"
      ],
      "metadata": {
        "id": "wI3Z4nAt2BM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_aggl_clustering_contours(img3, n_clusters, labels_img3)"
      ],
      "metadata": {
        "id": "tfkL0cCB2BUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Superpixels\n",
        "\n"
      ],
      "metadata": {
        "id": "TwjVQz18LfM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will work with the SLIC superpixels. Reference example: https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_segmentations.html#sphx-glr-auto-examples-segmentation-plot-segmentations-py"
      ],
      "metadata": {
        "id": "W397IUQC2knk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from skimage.segmentation import slic\n",
        "\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from skimage import img_as_ubyte\n",
        "\n",
        "# select the input images\n",
        "img1_slic = img1.copy()\n",
        "img2_slic = img2.copy()\n",
        "img3_slic = img3.copy()\n",
        "\n",
        "# run the algorithm for each image\n",
        "segments1_slic = slic(img1, n_segments=250, compactness=10, sigma=1,start_label=1)\n",
        "segments2_slic = slic(img2, n_segments=250, compactness=10, sigma=1,start_label=1)\n",
        "segments3_slic = slic(img3, n_segments=250, compactness=10, sigma=1,start_label=1)\n",
        "\n",
        "print(f'SLIC number of segments: {len(np.unique(segments1_slic))}')\n",
        "print(f'SLIC number of segments: {len(np.unique(segments2_slic))}')\n",
        "print(f'SLIC number of segments: {len(np.unique(segments3_slic))}')\n",
        "\n",
        "# draw the boundaries in the image\n",
        "img1_slic = mark_boundaries(img1_slic, segments1_slic)\n",
        "img2_slic = mark_boundaries(img2_slic, segments2_slic)\n",
        "img3_slic = mark_boundaries(img3_slic, segments3_slic)\n",
        "\n",
        "# go back to opencv format to display the image\n",
        "cv_slic_1 = img_as_ubyte(img1_slic)\n",
        "cv_slic_2 = img_as_ubyte(img2_slic)\n",
        "cv_slic_3 = img_as_ubyte(img3_slic)\n"
      ],
      "metadata": {
        "id": "PVfIpo042qCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(cv2.hconcat([cv_slic_1, cv_slic_2, cv_slic_3]))\n"
      ],
      "metadata": {
        "id": "auoWhDcK46Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Watersheed"
      ],
      "metadata": {
        "id": "L2N3RKBLLfP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference example:\n",
        "https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html"
      ],
      "metadata": {
        "id": "rhBpp1kt7b7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def watersheed_opencv(img):\n",
        "  watersheed_img = img.copy()\n",
        "  gray = cv2.cvtColor(watersheed_img,cv2.COLOR_BGR2GRAY)\n",
        "  ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "\n",
        "  # noise removal\n",
        "  kernel = np.ones((3,3),np.uint8)\n",
        "  opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations = 2)\n",
        "  # sure background area\n",
        "  sure_bg = cv2.dilate(opening,kernel,iterations=3)\n",
        "  # Finding sure foreground area\n",
        "  dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
        "  ret, sure_fg = cv2.threshold(dist_transform,0.7*dist_transform.max(),255,0)\n",
        "  # Finding unknown region\n",
        "  sure_fg = np.uint8(sure_fg)\n",
        "  unknown = cv2.subtract(sure_bg,sure_fg)\n",
        "\n",
        "\n",
        "  # Marker labelling\n",
        "  ret, markers = cv2.connectedComponents(sure_fg)\n",
        "  # Add one to all labels so that sure background is not 0, but 1\n",
        "  markers = markers+1\n",
        "  # Now, mark the region of unknown with zero\n",
        "  markers[unknown==255] = 0\n",
        "\n",
        "  markers = cv2.watershed(watersheed_img,markers)\n",
        "  watersheed_img[markers == -1] = [255,255,255]\n",
        "  return(watersheed_img)"
      ],
      "metadata": {
        "id": "tzca-zsk7cG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "watersheed_img1 = watersheed_opencv(img1)\n",
        "watersheed_img2 = watersheed_opencv(img2)\n",
        "watersheed_img3 = watersheed_opencv(img3)"
      ],
      "metadata": {
        "id": "xaFmUR648xi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(cv2.hconcat([watersheed_img1, watersheed_img2, watersheed_img3]))\n"
      ],
      "metadata": {
        "id": "0SEOnLeH8al6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an exercice, you can compare the watersheed results obtained with opencv with the ones obtained with scikit-image:\n",
        "\n",
        "https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_segmentations.html#sphx-glr-auto-examples-segmentation-plot-segmentations-py\n",
        "\n",
        "Can you improve the results by changing parameters?"
      ],
      "metadata": {
        "id": "gut5g6Dz9ttc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph-based segmentation: Felzenszwalb\n",
        "\n"
      ],
      "metadata": {
        "id": "09d-iYB4LfS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference implementation: \n",
        "https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_segmentations.html#sphx-glr-auto-examples-segmentation-plot-segmentations-py"
      ],
      "metadata": {
        "id": "535iB-Gz-cPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from skimage.segmentation import felzenszwalb\n",
        "\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from skimage import img_as_ubyte\n",
        "\n",
        "# select the input images\n",
        "img1_felzenszwalb = img1.copy()\n",
        "img2_felzenszwalb = img2.copy()\n",
        "img3_felzenszwalb = img3.copy()\n",
        "\n",
        "# run the algorithm for each image\n",
        "segments1_felzenszwalb = felzenszwalb(img1_felzenszwalb, scale=100, sigma=0.5, min_size=50)\n",
        "segments2_felzenszwalb = felzenszwalb(img2_felzenszwalb, scale=100, sigma=0.5, min_size=50)\n",
        "segments3_felzenszwalb = felzenszwalb(img3_felzenszwalb, scale=100, sigma=0.5, min_size=50)\n",
        "\n",
        "print(f'felzenszwalb number of segments: {len(np.unique(segments1_felzenszwalb))}')\n",
        "print(f'felzenszwalb number of segments: {len(np.unique(segments2_felzenszwalb))}')\n",
        "print(f'felzenszwalb number of segments: {len(np.unique(segments3_felzenszwalb))}')\n",
        "\n",
        "# draw the boundaries in the image\n",
        "img1_felzenszwalb = mark_boundaries(img1_felzenszwalb, segments1_felzenszwalb)\n",
        "img2_felzenszwalb = mark_boundaries(img2_felzenszwalb, segments2_felzenszwalb)\n",
        "img3_felzenszwalb = mark_boundaries(img3_felzenszwalb, segments3_felzenszwalb)\n",
        "\n",
        "# go back to opencv format to display the image\n",
        "cv_felzenszwalb_1 = img_as_ubyte(img1_felzenszwalb)\n",
        "cv_felzenszwalb_2 = img_as_ubyte(img2_felzenszwalb)\n",
        "cv_felzenszwalb_3 = img_as_ubyte(img3_felzenszwalb)\n"
      ],
      "metadata": {
        "id": "S2hh0d0b-h4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(cv2.hconcat([cv_felzenszwalb_1, cv_felzenszwalb_2, cv_felzenszwalb_3]))\n"
      ],
      "metadata": {
        "id": "vFbn8WI0-1kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mean Shift\n",
        "\n",
        "Implementation using only color information (without spatial coordinates):"
      ],
      "metadata": {
        "id": "0op3L2Fr9tJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np    \n",
        "import cv2    \n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "\n",
        "def mean_shift_color(img_meanShift):\n",
        "\n",
        "  # Shape of original image    \n",
        "  originShape = img_meanShift.shape\n",
        "\n",
        "  # Converting image into array of dimension [nb of pixels in originImage, 3]\n",
        "  # based on r g b intensities    \n",
        "  flatImg=np.reshape(img_meanShift, [-1, 3])\n",
        "\n",
        "  # Estimate bandwidth for meanshift algorithm    \n",
        "  bandwidth = estimate_bandwidth(flatImg, quantile=0.1, n_samples=100)    \n",
        "  ms = MeanShift(bandwidth = bandwidth, bin_seeding=True)\n",
        "\n",
        "  # Performing meanshift on flatImg    \n",
        "  ms.fit(flatImg)\n",
        "\n",
        "  # (r,g,b) vectors corresponding to the different clusters after meanshift    \n",
        "  labels=ms.labels_\n",
        "\n",
        "  # Remaining colors after meanshift    \n",
        "  cluster_centers = ms.cluster_centers_    \n",
        "\n",
        "  # Finding and showing the number of clusters    \n",
        "  labels_unique = np.unique(labels)    \n",
        "  n_clusters_ = len(labels_unique)    \n",
        "  print(\"number of estimated clusters : %d\" % n_clusters_)    \n",
        "\n",
        "  # building final image:\n",
        "  segmentedImg = cluster_centers[np.reshape(labels, originShape[:2])]\n",
        "\n",
        "  return segmentedImg.astype(np.uint8)"
      ],
      "metadata": {
        "id": "Ssxyp2T9AebP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# executing function with the images\n",
        "img_meanShift1 = mean_shift_color(img1.copy())\n",
        "img_meanShift2 = mean_shift_color(img2.copy())\n",
        "img_meanShift3 = mean_shift_color(img3.copy())\n"
      ],
      "metadata": {
        "id": "bO5qjY_hKRMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying segmented image    \n",
        "cv2_imshow(cv2.hconcat([img_meanShift1, img_meanShift2, img_meanShift3]))\n"
      ],
      "metadata": {
        "id": "1SO_JTepJCBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proposed exercice: create a mean shift function that also uses the spatial coordinates"
      ],
      "metadata": {
        "id": "zOJDaU8eLfbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalized cuts"
      ],
      "metadata": {
        "id": "IyCUe9ceLNYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference code: https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_ncut.html\n"
      ],
      "metadata": {
        "id": "IrwMBzcqLVMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import data, segmentation, color\n",
        "from skimage.future import graph\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def normalized_cuts (input_image):\n",
        "\n",
        "  # we start doing a slic segmentation. It generates the set of initial regions (400)\n",
        "  labels1 = segmentation.slic(input_image, compactness=30, n_segments=400,start_label=1)\n",
        "\n",
        "  # lets build the output image just as a reference:\n",
        "  out1 = color.label2rgb(labels1, input_image, kind='avg', bg_label=0)\n",
        "\n",
        "  # normalized cuts needs a Region Adjacency Graph (RAG): a graph showing \n",
        "  # neighborhood relations between regions\n",
        "  g = graph.rag_mean_color(input_image, labels1, mode='similarity')\n",
        "\n",
        "  # lets apply the algorithm to our graph:\n",
        "  labels2 = graph.cut_normalized(labels1, g)\n",
        "\n",
        "  # and generate the output image:\n",
        "  out2 = color.label2rgb(labels2, input_image, kind='avg', bg_label=0)\n",
        "\n",
        "  return(out1, out2)\n",
        "\n"
      ],
      "metadata": {
        "id": "1Mz4veI-LUj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the algorithm with the images:\n",
        "ncuts_superpixel1, ncuts_result1 = normalized_cuts(img1.copy())\n",
        "ncuts_superpixel2, ncuts_result2 = normalized_cuts(img2.copy())\n",
        "ncuts_superpixel3, ncuts_result3 = normalized_cuts(img3.copy())\n"
      ],
      "metadata": {
        "id": "gskE6of6Mxmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(cv2.hconcat([ncuts_superpixel1, ncuts_superpixel2, ncuts_superpixel3]))\n"
      ],
      "metadata": {
        "id": "DgJsus7wLL4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(cv2.hconcat([ncuts_result1, ncuts_result2, ncuts_result3]))\n"
      ],
      "metadata": {
        "id": "b44Ym_EVL38X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S7HUFMAQNNBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}